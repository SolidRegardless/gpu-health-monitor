# GPU Health Monitor - Database Tables Explained

**Database:** gpu_health  
**Generated:** 2026-02-12 09:38 GMT

## ğŸ“Š Table Categories

All tables share the same PostgreSQL database (`gpu_health`) but belong to different systems:

---

## âœ… GPU Health Monitor Tables (Core Application)

These are YOUR application tables - actively used for GPU monitoring:

| Table | Purpose | Rows | Status |
|-------|---------|------|--------|
| **gpu_assets** | GPU inventory and metadata | 1 | âœ… Active |
| **gpu_metrics** | Time-series GPU telemetry (hypertable) | 630+ | âœ… Live stream |
| **gpu_health_scores** | Multi-dimensional health assessments | 6 | âœ… Updated every 15min |
| **gpu_features** | ML features for prediction models | 8 | âœ… Updated every 5min |
| **gpu_failure_predictions** | Failure probability forecasts | 7 | âœ… Updated every 5min |
| **gpu_economic_decisions** | NPV-based lifecycle recommendations | 5 | âœ… Updated every 30min |
| **anomalies** | ML-detected anomalies (temp, power, etc) | 66 | âœ… Updated every 5min |
| **gpu_failure_labels** | Ground truth labels for training | 0 | âœ… Empty (no failures yet) |

**Summary:** 8 tables, all working correctly âœ…

---

## ğŸ”¬ MLflow Tables (ML Experiment Tracking)

These tables belong to **MLflow** - an open-source ML platform for experiment tracking, model registry, and deployment.

### Experiment Tracking Tables

| Table | Purpose | Expected Use |
|-------|---------|--------------|
| **experiments** | ML experiment definitions | Log training experiments |
| **runs** | Individual training runs | Each model training session |
| **metrics** | Training metrics (accuracy, loss, etc) | Performance tracking |
| **params** | Hyperparameters used in training | Model configuration |
| **tags** | Metadata tags for runs/experiments | Organization/filtering |
| **latest_metrics** | Most recent metric value per run | Quick lookups |

### Model Registry Tables

| Table | Purpose | Expected Use |
|-------|---------|--------------|
| **registered_models** | Catalog of production models | Model versioning |
| **model_versions** | Specific model version instances | Deployment tracking |
| **model_version_tags** | Tags for model versions | Metadata |
| **registered_model_aliases** | Named aliases for versions (e.g., "production") | Deployment management |
| **registered_model_tags** | Tags for models | Organization |

### Dataset Tracking Tables

| Table | Purpose | Expected Use |
|-------|---------|--------------|
| **datasets** | Training/validation datasets | Data provenance |
| **inputs** | Input data references for runs | Reproducibility |
| **input_tags** | Tags for input datasets | Metadata |

### Other MLflow Tables

| Table | Purpose | Expected Use |
|-------|---------|--------------|
| **experiment_tags** | Tags for experiments | Organization |
| **alembic_version** | Database migration version | Schema management |

**Summary:** 15 tables, all empty âœ… **This is expected and correct!**

---

## ğŸ¤” Why Are MLflow Tables Empty?

MLflow tables are empty because **no ML experiments have been logged yet**. They will populate when you:

1. **Train new models** using MLflow tracking:
   ```python
   import mlflow
   
   with mlflow.start_run():
       mlflow.log_param("learning_rate", 0.01)
       mlflow.log_metric("accuracy", 0.95)
       mlflow.sklearn.log_model(model, "failure_predictor")
   ```

2. **Register models** for production:
   ```python
   mlflow.register_model(
       model_uri="runs:/abc123/model",
       name="gpu_failure_predictor"
   )
   ```

3. **View experiments** in the MLflow UI:
   - http://localhost:5000

---

## ğŸ¯ When Would You Use MLflow?

### Current State (Working)
Your system currently uses **pre-trained models** loaded from pickle files:
- `failure_predictor.pkl` - XGBoost failure prediction model
- Models are loaded at service startup
- No experiment tracking needed for inference

### Future Use Cases (Optional)

1. **Model Retraining Pipeline**
   - Collect failure data over time
   - Train new models on fresh data
   - Log training metrics and parameters
   - Compare model performance across versions

2. **A/B Testing Models**
   - Deploy multiple model versions
   - Track which performs better
   - Promote best model to production

3. **Model Provenance**
   - Track what data was used for training
   - Record hyperparameters and results
   - Enable reproducibility

4. **Team Collaboration**
   - Share experiments with team members
   - Compare different approaches
   - Document model improvements

---

## ğŸ—ï¸ Database Architecture Summary

```
gpu_health database (PostgreSQL + TimescaleDB)
â”‚
â”œâ”€â”€ GPU Health Monitor Schema (8 tables)
â”‚   â”œâ”€â”€ gpu_assets (inventory)
â”‚   â”œâ”€â”€ gpu_metrics (time-series hypertable) âš¡ High frequency
â”‚   â”œâ”€â”€ gpu_health_scores
â”‚   â”œâ”€â”€ gpu_features
â”‚   â”œâ”€â”€ gpu_failure_predictions
â”‚   â”œâ”€â”€ gpu_economic_decisions
â”‚   â”œâ”€â”€ anomalies
â”‚   â””â”€â”€ gpu_failure_labels
â”‚
â””â”€â”€ MLflow Schema (15 tables)
    â”œâ”€â”€ Experiment Tracking (experiments, runs, metrics, params, tags)
    â”œâ”€â”€ Model Registry (registered_models, model_versions, aliases)
    â”œâ”€â”€ Dataset Tracking (datasets, inputs, input_tags)
    â””â”€â”€ Metadata (experiment_tags, alembic_version)
```

---

## ğŸ’¡ Should You Remove MLflow Tables?

**No!** Keep them because:

1. âœ… **They're harmless** - Empty tables use minimal space
2. âœ… **Future-ready** - You might want ML tracking later
3. âœ… **No overhead** - They don't impact performance
4. âœ… **Proper architecture** - Experiment tracking is a best practice

If you **really** wanted to remove them (not recommended):
```bash
# Stop MLflow service
docker compose stop mlflow

# Remove from docker-compose.yml
# Tables would remain but be unused
```

---

## âœ… Recommendations

### For Development/Testing (Current)
**Keep everything as is** - MLflow tables are ready if needed, causing no issues.

### For Production (Future)
**Separate databases** might be cleaner:
- `gpu_health` - Application data (metrics, health scores, etc.)
- `mlflow` - ML experiment tracking (separate database)

But this is optional and only needed at scale.

---

## ğŸ“‹ Quick Reference: Which Tables Are Mine?

**Your Application Tables (8):**
- `gpu_*` prefix â†’ Your GPU monitoring system
- All actively used and populated

**MLflow Tables (15):**
- No prefix (runs, experiments, metrics, etc.) â†’ MLflow system
- Empty until you start logging ML experiments
- Correct and expected behavior

**Summary:** 23 total tables, all correctly configured âœ…
